{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "    ## 流程 = 語音接收-->儲存-->mfcc轉換-->判別--回傳結果 ##\n",
    "    1. 音檔必須大於10s的版本\n",
    "    2. 訓練資料:train_v7.json\n",
    "    3. 權重:model_version_v7.h5\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import math\n",
    "import struct\n",
    "import base64\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import wave\n",
    "from flask import Flask, jsonify, request\n",
    "from flask_cors import cross_origin\n",
    "from keras.models import load_model\n",
    "import speech_recognition as sr\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import speech_recognition as sr\n",
    "from googletrans import Translator\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.fasttext import FastText\n",
    "model = FastText.load_fasttext_format('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw\n",
    "def oscillogram_spectrum():\n",
    "    \"\"\"\n",
    "    画出音频文件audio_path的声波图和频谱图\n",
    "    :param audio_path:音频文件路径\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 读取wav文件\n",
    "    filename = \"./audio.wav\"\n",
    "    wavefile = wave.open(filename, 'r')  # open for writing\n",
    "    # 读取wav文件的四种信息的函数。numframes表示一共读取了几个frames。\n",
    "    nchannels = wavefile.getnchannels()\n",
    "    sample_width = wavefile.getsampwidth()\n",
    "    framerate = wavefile.getframerate()\n",
    "    numframes = wavefile.getnframes()\n",
    "    y = np.zeros(numframes)\n",
    "\n",
    "    for index, i in enumerate(range(numframes)):\n",
    "        val = wavefile.readframes(1)\n",
    "        left = val[0:2]\n",
    "        # right = val[2:4]\n",
    "        v = struct.unpack('h', left)[0]\n",
    "        # print(\"v\", v)\n",
    "        y[i] = v\n",
    "    # framerate就是声音的采用率，文件初读取的值。\n",
    "    Fs = framerate\n",
    "    time = np.arange(0, numframes) * (1.0 / framerate)\n",
    "    feature=[]\n",
    "    feature_y=[]\n",
    "    dict_={\n",
    "        \"y\":y\n",
    "    }\n",
    "\n",
    "    for i, x in enumerate(time):\n",
    "        if y[i]>=500 or y[i]<=-500:\n",
    "            feature.append(x)\n",
    "            feature_y.append(y[i])\n",
    "\n",
    "\n",
    "    time_cut = round(numframes/4)\n",
    "    new_time_cut = [word for word in time if word != 0.0]\n",
    "    # print(new_time_cut)\n",
    "    feature = np.array(feature)\n",
    "    feature_y = np.array(feature_y)\n",
    "\n",
    "#     time1 = time[ :time_cut]\n",
    "#     time2 = time[time_cut : time_cut*2]\n",
    "#     time3 = time[time_cut*2 : time_cut*3]\n",
    "#     time4 = time[time_cut*3:]\n",
    "\n",
    "    # 显示时域图(波形图)\n",
    "    # plt.subplot(2,2,1)\n",
    "    plt.subplot(1,1,1)\n",
    "    a,= plt.plot(feature, feature_y, color='r')\n",
    "    # plt.subplot(2,2,2)\n",
    "    # plt.subplot(1,1,1)\n",
    "    b, = plt.plot(time, y)\n",
    "\n",
    "    plt.legend([a , b], [\"Teacher Voice\", \"Your Voice\"])\n",
    "    # plt.subplot(1,1,1)\n",
    "    # plt.subplot(2,2,1)\n",
    "    # plt.plot(time1, y[ :time_cut])\n",
    "    #\n",
    "    # plt.subplot(2, 2, 2)\n",
    "    # plt.plot(time2, y[time_cut : time_cut*2])\n",
    "    #\n",
    "    # plt.subplot(2, 2, 3)\n",
    "    # plt.plot(time3, y[time_cut*2 : time_cut*3])\n",
    "    #\n",
    "    # plt.subplot(2, 2, 4)\n",
    "    # plt.plot(time4, y[time_cut*3:])\n",
    "    \n",
    "    # 显示频域图(频谱图)\n",
    "    # plt.subplot(2,2,3)\n",
    "    # plt.specgram(y, NFFT=1024, Fs=Fs, noverlap=900)\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.5, wspace=0.3)\n",
    "    plt.rcParams['savefig.dpi'] = 100\n",
    "    plt.rcParams['figure.dpi'] = 100\n",
    "    plt.ylabel('amplitude(db)',fontsize=10)\n",
    "    plt.xlabel('Time(s)',fontsize=10)\n",
    "    plt.savefig('./audio.png')\n",
    "    plt.show()\n",
    "    \n",
    "def Bargraph():\n",
    "    plt.plot(point_list, color=\"b\", linewidth=3)\n",
    "    plt.figsize=(12,8)\n",
    "    plt.title('Mr. XXX Growth Record',fontsize=20)\n",
    "    plt.xlabel('date',fontsize=12)\n",
    "    plt.ylabel('point',fontsize=12)\n",
    "    plt.xtick(fontsize=10)\n",
    "    plt.ytick(fontsize=10)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "def get_wave_word(file_path):\n",
    "    r = sr.Recognizer()                        #預設辨識英文\n",
    "    with sr.WavFile(file_path) as source:  #讀取wav檔\n",
    "        audio = r.record(source)\n",
    "    try:\n",
    "        return \"Transcription: \" + r.recognize_google(audio,language=\"en\")\n",
    "                                              #使用Google的服務\n",
    "    except :\n",
    "        return \"請再唸一次\"\n",
    "    \n",
    "## 11/30新增\n",
    "def check_your_voice_Volume():\n",
    "    normal_sound=0\n",
    "    too_low_sound=0\n",
    "    for index, i in enumerate(range(numframes)):\n",
    "    val = wavefile.readframes(1)\n",
    "    left = val[0:2]\n",
    "    # right = val[2:4]\n",
    "    v = struct.unpack('h', left)[0]\n",
    "    y[i] = v\n",
    "    if y[i] >=500 or y[i]<=-500:\n",
    "        normal_sound+=1\n",
    "    else:\n",
    "        too_low_sound+=1\n",
    "    if normal_sound >= int(numframes*0.3):\n",
    "        print('音量正常', normal_sound, too_low_sound)\n",
    "    else:\n",
    "        print('音量太小', normal_sound, too_low_sound)\n",
    "        response_dict={\n",
    "            \"response\":\"Your voice is too low\"\n",
    "        }\n",
    "        response = json.dumps(response_dict)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level1=0\n",
    "level2=0\n",
    "level3=0\n",
    "level4=0\n",
    "level5=0\n",
    "\n",
    "def check_your_audio():\n",
    "    # 读取wav文件\n",
    "    filename = \"./audio.wav\"\n",
    "    wavefile = wave.open(filename, 'r')  # open for writing\n",
    "    # 读取wav文件的四种信息的函数。numframes表示一共读取了几个frames。\n",
    "    nchannels = wavefile.getnchannels()\n",
    "    sample_width = wavefile.getsampwidth()\n",
    "    framerate = wavefile.getframerate()\n",
    "    numframes = wavefile.getnframes()\n",
    "\n",
    "    y = np.zeros(numframes)\n",
    "    return numframes, y, wavefile\n",
    "            \n",
    "def load_data_test(data_path):\n",
    "    with open(data_path, \"r\",encoding=\"utf-8\") as fp:\n",
    "        data = json.load(fp)\n",
    "\n",
    "    X = np.array(data[\"mfcc\"])\n",
    "    y = np.array(data[\"labels\"])\n",
    "    X = X[..., np.newaxis]\n",
    "    X = X[:,np.newaxis]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def score_(point):\n",
    "    global level1, level2, level3, level4, level5\n",
    "    if point >=0 and point <=20:\n",
    "        level1+=1\n",
    "        return ('你的美式口音還需要多加強')\n",
    "    elif point >=21 and point <=40:\n",
    "        level2+=1\n",
    "        return ('你的美式口音有抓到一點訣竅，但還有很多細節沒有到位，多多練習會更好哦')\n",
    "    elif point >=41 and point <=60:\n",
    "        level3+=1\n",
    "        return ('你的美式口音表現很不錯, 但一些口音細節還需要加強')\n",
    "    elif point >=61 and point <=80:\n",
    "        level4+=1\n",
    "        return ('你的美式口音整理表現得很好、細節也有到位, 多練習就完美囉 ')\n",
    "    elif point >=81 and point <=100:\n",
    "        level5+=1\n",
    "        return ('你的美式口英非常標準, 恭喜你')\n",
    "\n",
    "def prediction(X, model_load):\n",
    "    score_sum=0\n",
    "    name_score=0\n",
    "    point_list=[]\n",
    "    for i, pre in enumerate(X):\n",
    "        prediction = model_load.predict(pre)\n",
    "        # print(\"prediction: \",prediction)\n",
    "        predictied_index = np.argmax(prediction, axis=1)\n",
    "        if int(predictied_index[0]) == 0:\n",
    "            predictied_index = np.argmin(prediction, axis=1)\n",
    "        score = (prediction[0][predictied_index])\n",
    "        # # print(i, predictied_index)\n",
    "#         print(\"prediction: \", prediction)\n",
    "        score_sum += score\n",
    "        if (i+1)%4==0:\n",
    "            print((score_sum/4)*100)\n",
    "            response = int((score_sum/4)*100)\n",
    "            point_list.append(response)\n",
    "            score_sum=0\n",
    "            name_score+=1\n",
    "    return score_(response)\n",
    "\n",
    "# PATH\n",
    "def get_mfcc():\n",
    "    data = {\n",
    "    \"name\":[],\n",
    "    \"labels\": [],\n",
    "    \"mfcc\": []\n",
    "    }\n",
    "    JSON_PATH = \"./audio.json\"\n",
    "    SAMPLE_RATE = 22050\n",
    "    TRACK_DURATION = 10  # measured in seconds   ###### 10 、16秒兩個版本 ######\n",
    "    SAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATION\n",
    "\n",
    "    # PARAMS\n",
    "    num_mfcc=13\n",
    "    n_fft=2048\n",
    "    hop_length=512\n",
    "    num_segments=4\n",
    "    samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)\n",
    "    num_mfcc_vectors_per_segment = math.ceil(samples_per_segment / hop_length)\n",
    "\n",
    "    # user files transform\n",
    "    file = \"./audio.wav\"\n",
    "\n",
    "    signal, sample_rate = librosa.load(file, sr=SAMPLE_RATE)\n",
    "\n",
    "    # process all segments of audio file\n",
    "    for id, d in enumerate(range(num_segments)):\n",
    "        # calculate start and finish sample for current segment\n",
    "        start = samples_per_segment * d\n",
    "        finish = start + samples_per_segment\n",
    "\n",
    "        # extract mfcc\n",
    "        mfcc = librosa.feature.mfcc(signal[start:finish], sample_rate, n_mfcc=num_mfcc, n_fft=n_fft,\n",
    "                                    hop_length=hop_length)\n",
    "        mfcc = mfcc.T\n",
    "        mfcc_cut = mfcc.copy()\n",
    "        mfcc_cut[abs(mfcc_cut)<10]=0\n",
    "        # store only mfcc feature with expected number of vectors\n",
    "        if len(mfcc_cut) == num_mfcc_vectors_per_segment:\n",
    "            # print(mfcc)\n",
    "            name = file.split(\"./\")[1].split(\".wav\")[0]\n",
    "            if (id+1)%4 == 0:\n",
    "                data[\"name\"].append(name)\n",
    "            data[\"mfcc\"].append(mfcc.tolist())\n",
    "            data[\"labels\"].append(0)\n",
    "\n",
    "    with open(\"audio.json\", \"w\",encoding=\"utf-8\") as fp:\n",
    "        json.dump(data, fp, indent=4)\n",
    "\n",
    "    return \"MFCC OK\"\n",
    "\n",
    "def back_img():\n",
    "    img_path=\"./audio.png\"\n",
    "    with open(img_path, 'rb') as img_f:\n",
    "        img_stream = img_f.read()\n",
    "        img_stream = base64.b64encode(img_stream)\n",
    "    return img_stream\n",
    "\n",
    "def plot_picture():\n",
    "    plt.plot(point_list, color=\"b\", linewidth=3)\n",
    "    plt.figsize=(12,8)\n",
    "    plt.title('Mr. XXX Growth Record',fontsize=20)\n",
    "    plt.xlabel('date',fontsize=12)\n",
    "    plt.ylabel('point',fontsize=12)\n",
    "    plt.xtick(fontsize=10)\n",
    "    plt.ytick(fontsize=10)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "@app.route('/file', methods=['GET', \"POST\"])\n",
    "@cross_origin()\n",
    "def files():\n",
    "    normal_sound=0\n",
    "    too_low_sound=0\n",
    "    file = request.files['file']\n",
    "    file.save('./audio.wav')\n",
    "    ##### 11/30新增\n",
    "    teacher = get_wave_word(file_path)\n",
    "    student = get_wave_word('audio.wav')\n",
    "    point = model.wv.similarity(teacher, student)\n",
    "    print(\"你的語意分數是: \", point)\n",
    "    if point <= 0.8:\n",
    "        return \"妳唸得不太正確, 請重新再唸一次!!\"\n",
    "    #####\n",
    "    check_your_voice_Volume()\n",
    "    numframes, y, wavefile = check_your_audio()\n",
    "    for index, i in enumerate(range(numframes)):\n",
    "        val = wavefile.readframes(1)\n",
    "        left = val[0:2]\n",
    "        # right = val[2:4]\n",
    "        v = struct.unpack('h', left)[0]\n",
    "        y[i] = v\n",
    "        if y[i] >=500 or y[i]<=-500:\n",
    "            normal_sound+=1\n",
    "        else:\n",
    "            too_low_sound+=1\n",
    "    if normal_sound >= int(numframes*0.3):\n",
    "        print('音量正常', normal_sound, too_low_sound)\n",
    "        response_dict={\n",
    "            \"response\":\"Voice OK\"\n",
    "        }\n",
    "        response = json.dumps(response_dict)\n",
    "        return response\n",
    "    else:\n",
    "        print('音量太小', normal_sound, too_low_sound)\n",
    "        response_dict={\n",
    "            \"response\":\"Your voice is too low\"\n",
    "        }\n",
    "        response = json.dumps(response_dict)\n",
    "        return response\n",
    "    get_mfcc()\n",
    "    TEST_PATH = \"./audio.json\"\n",
    "    X, y = load_data_test(TEST_PATH)\n",
    "    # 16s \"model_version_v7.h5\"\n",
    "    model_load = load_model(\"model/New 0.341 - 0.9212.h5\")\n",
    "    response = prediction(X, model_load)\n",
    "    print(response)\n",
    "    # picture\n",
    "    oscillogram_spectrum()\n",
    "    Bargraph()\n",
    "    picture = back_img()\n",
    "    re = {\n",
    "        \"response\" : response,\n",
    "        \"picture\" : picture.decode('utf-8')\n",
    "    }\n",
    "    re = json.dumps(re)\n",
    "    return re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0',port = 5003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取wav文件\n",
    "filename = \"./audio.wav\"\n",
    "wavefile = wave.open(filename, 'r')  # open for writing\n",
    "# 读取wav文件的四种信息的函数。numframes表示一共读取了几个frames。\n",
    "nchannels = wavefile.getnchannels()\n",
    "sample_width = wavefile.getsampwidth()\n",
    "framerate = wavefile.getframerate()\n",
    "numframes = wavefile.getnframes()\n",
    "\n",
    "y = np.zeros(numframes)\n",
    "\n",
    "print(nchannels)\n",
    "print(sample_width)\n",
    "print(framerate)\n",
    "print(numframes)\n",
    "\n",
    "s = numframes / framerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取wav文件\n",
    "filename = \"./audio.wav\"\n",
    "y, sr = librosa.load(filename, sr=None)\n",
    "print(y, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = librosa.feature.melspectrogram(y, sr, n_fft=1024, hop_length=512, n_mels=128)\n",
    "logmelspc = librosa.power_to_db(feature)\n",
    "logmelspc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "mfccs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "r = sr.Recognizer()                        #預設辨識英文\n",
    "with sr.WavFile(\"C:/Users/PC06/Downloads/lili colins/1/Lily Collins Discusses Filming Tolkien and Ted Bundy Biopic This Morning_1.wav\") as source:  #讀取wav檔\n",
    "    audio = r.record(source)\n",
    "try:\n",
    "    print(\"Transcription: \" + r.recognize_google(audio,language=\"en\"))\n",
    "                                          #使用Google的服務\n",
    "except LookupError:\n",
    "    print(\"Could not understand audio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wave_word():\n",
    "    r = sr.Recognizer()                        #預設辨識英文\n",
    "    with sr.WavFile(\"./voice/Will Smith 7.wav\") as source:  #讀取wav檔\n",
    "        audio = r.record(source)\n",
    "    try:\n",
    "        print(\"Transcription: \" , r.recognize_google(audio,language=\"en\"))\n",
    "                                              #使用Google的服務\n",
    "    except:\n",
    "         print(\"請再唸一次\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_wave_word()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
